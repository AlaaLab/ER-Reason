{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ACUITY PREDICTION FOR GPT SERIES: 3.5 AND 4o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)  # Display all rows\n",
    "pd.set_option('display.max_columns', None)  # Display all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Set max column width to None\n",
    "pd.set_option('display.width', None)  # Set width to None\n",
    "df = pd.read_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import time\n",
    "import urllib.parse\n",
    "\n",
    "pd.set_option('display.max_rows', None)  # Display all rows\n",
    "pd.set_option('display.max_columns', None)  # Display all columns\n",
    "pd.set_option('display.max_colwidth', None)  # Set max column width to None\n",
    "pd.set_option('display.width', None)  # Set width to None\n",
    "\n",
    "API_KEY = 'x'  ##### Paste your API key between the quotes #####\n",
    "API_VERSION = '2024-06-01'  # For the most recent production release: https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation#latest-ga-api-release\n",
    "RESOURCE_ENDPOINT = 'x'  \n",
    "DEPLOYMENT_NAME = 'gpt-35-turbo' ## change this to gpt-4o deployment name and rerun cells to see 4o acuity prediction results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Function to predict acuity with robust error handling\n",
    "def predict_acuity_with_all_info(row, max_retries=5):\n",
    "    \"\"\"\n",
    "    Predicts acuity level using all available information with robust error handling.\n",
    "    \"\"\"\n",
    "    # Skip if any critical field is missing\n",
    "    required_fields = ['primarychiefcomplaintname']\n",
    "    for field in required_fields:\n",
    "        if field not in row or pd.isna(row[field]):\n",
    "            return None\n",
    "    \n",
    "    # Build the prompt with all available information\n",
    "    prompt = \"Predict the emergency department acuity level for this patient.\\n\\n\"\n",
    "    \n",
    "    # Add chief complaint (required)\n",
    "    prompt += f\"Chief Complaint: {row['primarychiefcomplaintname']}\\n\"\n",
    "    \n",
    "    # Add optional fields if available\n",
    "    if 'age' in row and not pd.isna(row['age']):\n",
    "        prompt += f\"Age: {row['age']}\\n\"\n",
    "    \n",
    "    if 'sex' in row and not pd.isna(row['sex']):\n",
    "        prompt += f\"Sex: {row['sex']}\\n\"\n",
    "    \n",
    "    if 'firstrace' in row and not pd.isna(row['firstrace']):\n",
    "        prompt += f\"Race: {row['firstrace']}\\n\"\n",
    "    \n",
    "    if 'Vital_Signs' in row and not pd.isna(row['Vital_Signs']):\n",
    "        # Limit vital signs to 300 characters to reduce payload size\n",
    "        vital_signs = str(row['Vital_Signs'])\n",
    "        if len(vital_signs) > 300:\n",
    "            vital_signs = vital_signs[:300] + \"...\"\n",
    "        prompt += f\"Vital Signs: {vital_signs}\\n\"\n",
    "    \n",
    "    # Add instructions for output format\n",
    "    prompt += \"\\nSelect the most appropriate acuity level from the following options ONLY:\\n\"\n",
    "    prompt += \"'Immediate', 'Emergent', 'Urgent', 'Less Urgent', 'Non-Urgent'\\n\\n\"\n",
    "    prompt += \"Respond with ONLY ONE of these five options.\"\n",
    "    \n",
    "    # API request with retries and backoff\n",
    "    current_retry = 0\n",
    "    backoff_time = 2  # Initial backoff in seconds\n",
    "    \n",
    "    while current_retry <= max_retries:\n",
    "        try:\n",
    "            url = f\"{RESOURCE_ENDPOINT}/openai/deployments/{DEPLOYMENT_NAME}/chat/completions?api-version={API_VERSION}\"\n",
    "            \n",
    "            headers = {\n",
    "                \"Content-Type\": \"application/json\",\n",
    "                \"api-key\": API_KEY\n",
    "            }\n",
    "            \n",
    "            payload = {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"You are an experienced Emergency Department triage nurse.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"temperature\": 0.1,\n",
    "                \"max_tokens\": 50\n",
    "            }\n",
    "            \n",
    "            # Use increased timeout\n",
    "            response = requests.post(url, headers=headers, json=payload, timeout=60)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Extract and clean prediction\n",
    "            prediction = response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            \n",
    "            # Handle potential variations in responses\n",
    "            for acuity in ['Immediate', 'Emergent', 'Urgent', 'Less Urgent', 'Non-Urgent']:\n",
    "                if acuity.lower() in prediction.lower():\n",
    "                    return acuity\n",
    "            \n",
    "            # If no match found, return the raw response for debugging\n",
    "            return prediction\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            current_retry += 1\n",
    "            \n",
    "            if current_retry > max_retries:\n",
    "                print(f\"Failed after {max_retries} retries: {e}\")\n",
    "                return \"Prediction failed\"\n",
    "            \n",
    "            # Log the error and retry info\n",
    "            print(f\"Request failed: {e}. Retrying {current_retry}/{max_retries} after {backoff_time} seconds...\")\n",
    "            \n",
    "            # Implement exponential backoff with jitter\n",
    "            time.sleep(backoff_time + random.uniform(0, 1))\n",
    "            backoff_time *= 2  # Double the backoff time for next retry\n",
    "    \n",
    "    return \"Prediction failed\"\n",
    "\n",
    "# Process data in batches with checkpointing\n",
    "def process_with_checkpoints(df, batch_size=20, checkpoint_file=\"35_acuity_predictions_all_info.csv\"):\n",
    "    # Check if checkpoint exists\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        processed_df = pd.read_csv(checkpoint_file)\n",
    "        # Create a set of already processed encounter IDs for faster lookups\n",
    "        if 'encounterkey' in processed_df.columns:\n",
    "            processed_ids = set(processed_df['encounterkey'].tolist())\n",
    "        else:\n",
    "            # If no encounterkey column, use row indices\n",
    "            processed_ids = set(range(len(processed_df)))\n",
    "        print(f\"Resuming from checkpoint with {len(processed_ids)} already processed records\")\n",
    "    else:\n",
    "        processed_df = pd.DataFrame(columns=df.columns.tolist() + ['predicted_acuity'])\n",
    "        processed_ids = set()\n",
    "    \n",
    "    # Calculate total batches\n",
    "    total_records = len(df)\n",
    "    total_batches = (total_records + batch_size - 1) // batch_size\n",
    "    \n",
    "    # Track overall progress\n",
    "    processed_count = len(processed_ids)\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Process in batches\n",
    "    for batch_num in range(total_batches):\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min(start_idx + batch_size, total_records)\n",
    "        \n",
    "        # Get current batch\n",
    "        batch = df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        # Filter out already processed records\n",
    "        if 'encounterkey' in batch.columns:\n",
    "            batch = batch[~batch['encounterkey'].isin(processed_ids)]\n",
    "        else:\n",
    "            batch = batch.iloc[[i for i in range(start_idx, end_idx) if i not in processed_ids]]\n",
    "        \n",
    "        if len(batch) == 0:\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing batch {batch_num+1}/{total_batches}, records {start_idx}-{end_idx}\")\n",
    "        \n",
    "        # Process each record in batch\n",
    "        for idx, row in tqdm(batch.iterrows(), total=len(batch)):\n",
    "            # Make prediction\n",
    "            prediction = predict_acuity_with_all_info(row)\n",
    "            \n",
    "            # Add prediction to row\n",
    "            row_copy = row.copy()\n",
    "            row_copy['predicted_acuity'] = prediction\n",
    "            \n",
    "            # Append to results dataframe\n",
    "            processed_df = pd.concat([processed_df, pd.DataFrame([row_copy])], ignore_index=True)\n",
    "            \n",
    "            # Mark as processed\n",
    "            if 'encounterkey' in row:\n",
    "                processed_ids.add(row['encounterkey'])\n",
    "            else:\n",
    "                processed_ids.add(idx)\n",
    "            \n",
    "            # Update progress count\n",
    "            processed_count += 1\n",
    "        \n",
    "        # Save checkpoint after each batch\n",
    "        processed_df.to_csv(checkpoint_file, index=False)\n",
    "        \n",
    "        # Calculate and display progress statistics\n",
    "        elapsed_time = time.time() - start_time\n",
    "        records_per_second = processed_count / elapsed_time if elapsed_time > 0 else 0\n",
    "        estimated_remaining = (total_records - processed_count) / records_per_second if records_per_second > 0 else float('inf')\n",
    "        \n",
    "        print(f\"Progress: {processed_count}/{total_records} records ({processed_count/total_records:.1%})\")\n",
    "        print(f\"Speed: {records_per_second:.2f} records/second\")\n",
    "        print(f\"Est. time remaining: {estimated_remaining/60:.1f} minutes\")\n",
    "        print(f\"Checkpoint saved at: {checkpoint_file}\")\n",
    "        \n",
    "        # Add a short pause between batches to be nice to the API\n",
    "        time.sleep(2)\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "# Calculate accuracy\n",
    "def calculate_accuracy(df):\n",
    "    # Filter out rows with failed predictions\n",
    "    valid_df = df[df['predicted_acuity'].isin(['Immediate', 'Emergent', 'Urgent', 'Less Urgent', 'Non-Urgent'])]\n",
    "    \n",
    "    # Count matches\n",
    "    matches = (valid_df['predicted_acuity'] == valid_df['acuitylevel']).sum()\n",
    "    total = len(valid_df)\n",
    "    \n",
    "    accuracy = matches / total if total > 0 else 0\n",
    "    print(f\"Accuracy: {matches}/{total} = {accuracy:.4f} ({accuracy:.2%})\")\n",
    "    \n",
    "    # Count by acuity level\n",
    "    print(\"\\nAccuracy by acuity level:\")\n",
    "    for level in ['Immediate', 'Emergent', 'Urgent', 'Less Urgent', 'Non-Urgent']:\n",
    "        level_df = valid_df[valid_df['acuitylevel'] == level]\n",
    "        if len(level_df) > 0:\n",
    "            level_matches = (level_df['predicted_acuity'] == level_df['acuitylevel']).sum()\n",
    "            level_accuracy = level_matches / len(level_df)\n",
    "            print(f\"{level}: {level_matches}/{len(level_df)} = {level_accuracy:.4f} ({level_accuracy:.2%})\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the data\n",
    "    print(\"Loading data...\")\n",
    "    \n",
    "    print(f\"Loaded {len(df)} records\")\n",
    "    print(\"Starting prediction process with all information...\")\n",
    "    \n",
    "    # Run the prediction with all information\n",
    "    results_df = process_with_checkpoints(df)\n",
    "    \n",
    "    # Calculate and display accuracy\n",
    "    accuracy = calculate_accuracy(results_df)\n",
    "    \n",
    "    # Save final results\n",
    "    final_output = \"35_acuity_prediction_results_all_info.csv\"\n",
    "    results_df.to_csv(final_output, index=False)\n",
    "    print(f\"Final results saved to {final_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
