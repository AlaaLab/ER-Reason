{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from rouge_score import rouge_scorer\n",
    "import re\n",
    "import six \n",
    "from collections import Counter\n",
    "from evaluate import load\n",
    "\n",
    "df = pd.read_csv('4o_ehr_review.csv')\n",
    "df2 = pd.read_csv('35_ehr_review.csv')\n",
    "df3 = pd.read_csv('o3_ehr_review.csv')\n",
    "df4 = pd.read_csv('llama3_ehr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    # Ensure the text is a string, handle NaN or invalid data by converting it to a string or empty string\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)  # Convert to string if it's not a string (e.g., for float or NaN values)\n",
    "    \n",
    "    # Convert everything to lowercase.\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace any non-alpha-numeric characters with spaces.\n",
    "    text = re.sub(r\"[^a-z0-9]+\", \" \", six.ensure_str(text))\n",
    "    \n",
    "    return text\n",
    "\n",
    "df['Processed_One_Sentence_Extracted'] = df['One_Sentence_Extracted'].apply(process_text)\n",
    "df['Processed_Generated_Summary'] = df['Generated_Summary'].apply(process_text)\n",
    "\n",
    "# Similarly, you can process text in df2 or any other DataFrame\n",
    "df2['Processed_One_Sentence_Extracted'] = df2['One_Sentence_Extracted'].apply(process_text)\n",
    "df2['Processed_Generated_Summary'] = df2['Generated_Summary'].apply(process_text)\n",
    "\n",
    "# Similarly, you can process text in df2 or any other DataFrame\n",
    "df3['Processed_One_Sentence_Extracted'] = df3['One_Sentence_Extracted'].apply(process_text)\n",
    "df3['Processed_Generated_Summary'] = df3['Generated_Summary'].apply(process_text)\n",
    "\n",
    "\n",
    "# Similarly, you can process text in df2 or any other DataFrame\n",
    "df4['Processed_One_Sentence_Extracted'] = df4['Processed_One_Sentence_Extracted'].apply(process_text)\n",
    "df4['Processed_Generated_Summary'] = df4['Assistant_Content'].apply(process_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated ROUGE and F1 Scores for GPT-4o:\n",
      "ROUGE-1       0.346749\n",
      "ROUGE-2       0.126664\n",
      "ROUGE-L       0.285771\n",
      "ROUGE-1-F1    0.346749\n",
      "ROUGE-2-F1    0.126664\n",
      "ROUGE-L-F1    0.285771\n",
      "dtype: float64\n",
      "Aggregated ROUGE and F1 Scores for GPT-3.5:\n",
      "ROUGE-1       0.309570\n",
      "ROUGE-2       0.131968\n",
      "ROUGE-L       0.262131\n",
      "ROUGE-1-F1    0.309570\n",
      "ROUGE-2-F1    0.131968\n",
      "ROUGE-L-F1    0.262131\n",
      "dtype: float64\n",
      "Aggregated ROUGE and F1 Scores for O3-mini:\n",
      "ROUGE-1       0.311885\n",
      "ROUGE-2       0.120407\n",
      "ROUGE-L       0.271613\n",
      "ROUGE-1-F1    0.311885\n",
      "ROUGE-2-F1    0.120407\n",
      "ROUGE-L-F1    0.271613\n",
      "dtype: float64\n",
      "Aggregated ROUGE and F1 Scores for LLama3:\n",
      "ROUGE-1       0.293899\n",
      "ROUGE-2       0.110161\n",
      "ROUGE-L       0.250804\n",
      "ROUGE-1-F1    0.293899\n",
      "ROUGE-2-F1    0.110161\n",
      "ROUGE-L-F1    0.250804\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "## Define function to compute ROUGE scores and F1 scores\n",
    "def compute_rouge_scores(reference, hypothesis):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, hypothesis)\n",
    "    \n",
    "    # Extract ROUGE scores\n",
    "    rouge1_precision = scores['rouge1'].precision\n",
    "    rouge1_recall = scores['rouge1'].recall\n",
    "    rouge1_fmeasure = scores['rouge1'].fmeasure\n",
    "    \n",
    "    rouge2_precision = scores['rouge2'].precision\n",
    "    rouge2_recall = scores['rouge2'].recall\n",
    "    rouge2_fmeasure = scores['rouge2'].fmeasure\n",
    "    \n",
    "    rougeL_precision = scores['rougeL'].precision\n",
    "    rougeL_recall = scores['rougeL'].recall\n",
    "    rougeL_fmeasure = scores['rougeL'].fmeasure\n",
    "    \n",
    "    # Calculate F1 for each ROUGE score\n",
    "    f1_rouge1 = 2 * (rouge1_precision * rouge1_recall) / (rouge1_precision + rouge1_recall) if (rouge1_precision + rouge1_recall) != 0 else 0\n",
    "    f1_rouge2 = 2 * (rouge2_precision * rouge2_recall) / (rouge2_precision + rouge2_recall) if (rouge2_precision + rouge2_recall) != 0 else 0\n",
    "    f1_rougeL = 2 * (rougeL_precision * rougeL_recall) / (rougeL_precision + rougeL_recall) if (rougeL_precision + rougeL_recall) != 0 else 0\n",
    "    \n",
    "    # Return ROUGE F1 scores for the DataFrame\n",
    "    return pd.Series([rouge1_fmeasure, rouge2_fmeasure, rougeL_fmeasure, f1_rouge1, f1_rouge2, f1_rougeL], \n",
    "                     index=['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'ROUGE-1-F1', 'ROUGE-2-F1', 'ROUGE-L-F1'])\n",
    "\n",
    "\n",
    "\n",
    "# Compute ROUGE scores for df using processed columns\n",
    "df[['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'ROUGE-1-F1', 'ROUGE-2-F1', 'ROUGE-L-F1']] = df.apply(\n",
    "    lambda row: compute_rouge_scores(row['Processed_One_Sentence_Extracted'], row['Processed_Generated_Summary']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Compute ROUGE scores for df2 using processed columns\n",
    "df2[['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'ROUGE-1-F1', 'ROUGE-2-F1', 'ROUGE-L-F1']] = df2.apply(\n",
    "    lambda row: compute_rouge_scores(row['Processed_One_Sentence_Extracted'], row['Processed_Generated_Summary']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Compute ROUGE scores for df2 using processed columns\n",
    "df3[['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'ROUGE-1-F1', 'ROUGE-2-F1', 'ROUGE-L-F1']] = df3.apply(\n",
    "    lambda row: compute_rouge_scores(row['Processed_One_Sentence_Extracted'], row['Processed_Generated_Summary']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Compute ROUGE scores for df2 using processed columns\n",
    "df4[['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'ROUGE-1-F1', 'ROUGE-2-F1', 'ROUGE-L-F1']] = df4.apply(\n",
    "    lambda row: compute_rouge_scores(row['Processed_One_Sentence_Extracted'], row['Assistant_Content']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Compute the aggregated (mean) ROUGE scores and F1 scores for df\n",
    "aggregated_rouge_f1_scores_df = df[['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'ROUGE-1-F1', 'ROUGE-2-F1', 'ROUGE-L-F1']].mean()\n",
    "\n",
    "# Compute the aggregated (mean) ROUGE scores and F1 scores for df2\n",
    "aggregated_rouge_f1_scores_df2 = df2[['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'ROUGE-1-F1', 'ROUGE-2-F1', 'ROUGE-L-F1']].mean()\n",
    "\n",
    "# Compute the aggregated (mean) ROUGE scores and F1 scores for df2\n",
    "aggregated_rouge_f1_scores_df3 = df3[['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'ROUGE-1-F1', 'ROUGE-2-F1', 'ROUGE-L-F1']].mean()\n",
    "\n",
    "# Compute the aggregated (mean) ROUGE scores and F1 scores for df2\n",
    "aggregated_rouge_f1_scores_df4 = df4[['ROUGE-1', 'ROUGE-2', 'ROUGE-L', 'ROUGE-1-F1', 'ROUGE-2-F1', 'ROUGE-L-F1']].mean()\n",
    "\n",
    "# Print the aggregated scores for df\n",
    "print(f'Aggregated ROUGE and F1 Scores for GPT-4o:\\n{aggregated_rouge_f1_scores_df}')\n",
    "\n",
    "# Print the aggregated scores for df2\n",
    "print(f'Aggregated ROUGE and F1 Scores for GPT-3.5:\\n{aggregated_rouge_f1_scores_df2}')\n",
    "\n",
    "\n",
    "# Print the aggregated scores for df2\n",
    "print(f'Aggregated ROUGE and F1 Scores for O3-mini:\\n{aggregated_rouge_f1_scores_df3}')\n",
    "\n",
    "\n",
    "# Print the aggregated scores for df2\n",
    "print(f'Aggregated ROUGE and F1 Scores for LLama3:\\n{aggregated_rouge_f1_scores_df4}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text",
   "language": "python",
   "name": "text"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
