{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## GPT Series for EHR review - 4o and 3.5 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import base64\n",
    "import requests\n",
    "import time\n",
    "import urllib.parse\n",
    "from tqdm import tqdm  # For progress bar\n",
    "\n",
    "df = pd.read_csv('results.csv') ## er-reason csv file load here "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = 'x'  ##### Paste your API key between the quotes #####\n",
    "API_VERSION = '2024-06-01'  # For the most recent production release: https://learn.microsoft.com/en-us/azure/ai-services/openai/api-version-deprecation#latest-ga-api-release\n",
    "RESOURCE_ENDPOINT = 'x'  # no trailing slash--this is used by libraries as a partial URL\n",
    "DEPLOYMENT_NAME = \"gpt-35-turbo-16k\" ## replace with different deployment name for 4o "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get a summary from Azure OpenAI\n",
    "def get_summary(chief_complaint, discharge_summary, age, sex):\n",
    "    # Check if key fields are missing\n",
    "    if pd.isna(chief_complaint) or pd.isna(discharge_summary):\n",
    "        return None  # Skip if any key field is missing\n",
    "    \n",
    "    url = f\"{RESOURCE_ENDPOINT}/openai/deployments/{DEPLOYMENT_NAME}/chat/completions?api-version={API_VERSION}\"\n",
    "    \n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"api-key\": API_KEY\n",
    "    }\n",
    "    \n",
    "    payload = {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are an experienced emergency department (ED) physician creating a one-liner for a NEW patient who has just arrived at the ED. The patient's past medical records are available to you. Your task is to summarize the patient's relevant PAST medical history and end with their CURRENT chief complaint that is given with no adjectives about the chief complaint as you can NOT assume anything about their current condition. All notes and medical records provided are from PAST encounters, not the current visit.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Create a concise one-liner summary for a patient who has just arrived at the Emergency Department. The one-liner must:\\n\\n\"\n",
    "                                      f\"1. Start with demographic information (age, sex). Example of a one liner:  80 y.o. old male, with h/o of HFpEF (EF 55-60% 05/20/22), HTN, HLD, and bipolar disorder presenting with shortness of breath. \\n\"\n",
    "                                      f\"2. Include a concise summary of relevant PAST medical history from previous visits/notes\\n\"\n",
    "                                      f\"3. End with just CURRENT presenting chief complaint that is not capitilized in the summary and does have additional information regarding the chief complaint: '{chief_complaint}'\\n\\n\"\n",
    "                                      f\"IMPORTANT: Everything in the notes is from PAST encounters. The patient is NOW presenting with a NEW complaint: '{chief_complaint}'.\\n\\n\"\n",
    "                                      f\"Age: {age}\\n\"\n",
    "                                      f\"Sex: {sex}\\n\"\n",
    "                                      f\"PAST Medical Records:\\n{discharge_summary}\"}\n",
    "        ],\n",
    "        \"temperature\": 0.1,\n",
    "        \"max_tokens\": 4096\n",
    "    }\n",
    "    \n",
    "    retries = 0\n",
    "    max_retries = 5  # Increased from 3 to 5\n",
    "    backoff_factor = 2  # For exponential backoff\n",
    "    \n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = requests.post(url, headers=headers, json=payload)\n",
    "            \n",
    "            # Handle rate limiting (status code 429) or other 4xx errors\n",
    "            if response.status_code == 429 or (response.status_code >= 400 and response.status_code < 500):\n",
    "                wait_time = (backoff_factor ** retries) * 2  # Exponential backoff\n",
    "                print(f\"Rate limit hit or error {response.status_code}. Waiting for {wait_time} seconds before retry...\")\n",
    "                time.sleep(wait_time)\n",
    "                retries += 1\n",
    "                continue\n",
    "                \n",
    "            response.raise_for_status()  # Raise an error for other non-200 responses\n",
    "            return response.json()[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}. Retrying {retries+1}/{max_retries}...\")\n",
    "            wait_time = (backoff_factor ** retries) * 2  # Exponential backoff\n",
    "            time.sleep(wait_time)\n",
    "            retries += 1\n",
    "    \n",
    "    return None  # Return None if all retries fail\n",
    "\n",
    "# Function to process dataframe with checkpoint saving\n",
    "def process_dataframe_with_checkpoints(df, checkpoint_file=\"35_processing_checkpoint.json\", output_file=\"35_ehr_review.csv\", batch_size=10):\n",
    "    # Check if there's a checkpoint to resume from\n",
    "    start_index = 0\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        with open(checkpoint_file, 'r') as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "            start_index = checkpoint_data.get('last_processed_index', 0) + 1\n",
    "            print(f\"Resuming from index {start_index}\")\n",
    "            \n",
    "            # If there's a partially processed CSV, load it\n",
    "            if os.path.exists(output_file):\n",
    "                saved_df = pd.read_csv(output_file)\n",
    "                # Ensure it has the Generated_Summary column\n",
    "                if 'Generated_Summary' not in saved_df.columns:\n",
    "                    saved_df['Generated_Summary'] = None\n",
    "                # Transfer any already processed summaries\n",
    "                for idx in range(start_index):\n",
    "                    if idx < len(df) and idx < len(saved_df):\n",
    "                        if not pd.isna(saved_df.loc[idx, 'Generated_Summary']):\n",
    "                            df.loc[idx, 'Generated_Summary'] = saved_df.loc[idx, 'Generated_Summary']\n",
    "\n",
    "    # Initialize Generated_Summary column if it doesn't exist\n",
    "    if 'Generated_Summary' not in df.columns:\n",
    "        df['Generated_Summary'] = None\n",
    "    \n",
    "    # Process in batches with progress bar\n",
    "    total_rows = len(df)\n",
    "    progress_bar = tqdm(total=total_rows, initial=start_index, desc=\"Processing records\")\n",
    "    \n",
    "    for i in range(start_index, total_rows):\n",
    "        row = df.iloc[i]\n",
    "        \n",
    "        # Process the current row\n",
    "        summary = get_summary(\n",
    "            row[\"primarychiefcomplaintname\"], \n",
    "            row[\"Discharge_Summary_Text\"],\n",
    "            row[\"Age\"],\n",
    "            row[\"sex\"]\n",
    "        )\n",
    "        \n",
    "        # Update dataframe\n",
    "        df.loc[i, 'Generated_Summary'] = summary\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.update(1)\n",
    "        \n",
    "        # Add delay between API calls to prevent rate limiting\n",
    "        time.sleep(1)  # Wait 1 second between calls\n",
    "        \n",
    "        # Save checkpoint and intermediate results after each batch\n",
    "        if (i + 1) % batch_size == 0 or i == total_rows - 1:\n",
    "            # Save checkpoint\n",
    "            with open(checkpoint_file, 'w') as f:\n",
    "                json.dump({'last_processed_index': i}, f)\n",
    "            \n",
    "            # Save current results\n",
    "            df.to_csv(output_file, index=False)\n",
    "            print(f\"\\nCheckpoint saved at index {i}\")\n",
    "    \n",
    "    progress_bar.close()\n",
    "    print(f\"Processing complete. Results saved to {output_file}\")\n",
    "    \n",
    "    # Clean up checkpoint file when done\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        os.remove(checkpoint_file)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Apply the processing function to the dataframe\n",
    "df = process_dataframe_with_checkpoints(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
